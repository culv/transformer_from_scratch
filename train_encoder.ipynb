{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8068288,"sourceType":"datasetVersion","datasetId":4760357}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overfitting a small encoder-only classifier to a dataset. Not using a validation set since i'm mostly just interested in getting the transformer architecture training and not its actual performance or how well it generalizes on this problem","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-21T21:57:36.692205Z","iopub.execute_input":"2024-04-21T21:57:36.692865Z","iopub.status.idle":"2024-04-21T21:57:36.705602Z","shell.execute_reply.started":"2024-04-21T21:57:36.692830Z","shell.execute_reply":"2024-04-21T21:57:36.704626Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/input/ratings/Womens_Clothing_E-Commerce_Reviews_CLEANED.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%rm -r transformer_from_scratch\n!git clone https://github.com/culv/transformer_from_scratch.git\n%cd transformer_from_scratch","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:57:36.739777Z","iopub.execute_input":"2024-04-21T21:57:36.740238Z","iopub.status.idle":"2024-04-21T21:57:39.177297Z","shell.execute_reply.started":"2024-04-21T21:57:36.740212Z","shell.execute_reply":"2024-04-21T21:57:39.176169Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"rm: cannot remove 'transformer_from_scratch': No such file or directory\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'transformer_from_scratch'...\nremote: Enumerating objects: 72, done.\u001b[K\nremote: Counting objects: 100% (72/72), done.\u001b[K\nremote: Compressing objects: 100% (49/49), done.\u001b[K\nremote: Total 72 (delta 37), reused 55 (delta 23), pack-reused 0\u001b[K\nUnpacking objects: 100% (72/72), 91.76 KiB | 789.00 KiB/s, done.\n/kaggle/working/transformer_from_scratch/transformer_from_scratch\n","output_type":"stream"}]},{"cell_type":"code","source":"from data_utils import Gpt2Tokenizer\n\nt = Gpt2Tokenizer\nprint(len(t))\nprint(t.vocab_size)\nprint(t.eos_token)\nt.pad_token = t.eos_token\nprint(t.pad_token)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:57:39.179509Z","iopub.execute_input":"2024-04-21T21:57:39.179836Z","iopub.status.idle":"2024-04-21T21:57:39.195161Z","shell.execute_reply.started":"2024-04-21T21:57:39.179802Z","shell.execute_reply":"2024-04-21T21:57:39.194361Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"50257\n50257\n<|endoftext|>\n<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformer_from_scratch.data_utils import BalancedClassRandomSampler, PandasDataset, tokenize\n\ndata_path = \"/kaggle/input/ratings/Womens_Clothing_E-Commerce_Reviews_CLEANED.csv\"\nds = PandasDataset(\n    data_path,\n    input_column=\"review\",\n    output_column=\"Recommended IND\",\n    input_transform=tokenize(160)\n)\nprint(ds[111])","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:57:39.196592Z","iopub.execute_input":"2024-04-21T21:57:39.196948Z","iopub.status.idle":"2024-04-21T21:57:39.293472Z","shell.execute_reply.started":"2024-04-21T21:57:39.196923Z","shell.execute_reply":"2024-04-21T21:57:39.292309Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"((tensor([ 7279,  1324,  1563,   278,  3081,   383,  1486,    14, 43358,   286,\n          262,  6576,   389,  2407, 48259,    11,   781,  5893,   290, 23564,\n           13,   475,  1106,   612,   318,   645,   835,   326,   262,  6576,\n         1312,  2722,   318,   649,    13,   262,  3124,   318,   257, 24887,\n        18989,   503,  2266,   290,   612,   389,  2042, 43329,   477,   625,\n          262, 10999,  1989,    13,   612,   318,   645,  7621,   986,   262,\n         9664,  3073,  3102, 11081,   290, 22138,  1068,   290,   318,   407,\n        23453,    11, 15175,   393,   649,    13,  1312,   716,   845, 11679,\n          416,   262,  3081,   286,   262,  2378,   326,  1312,  2722,    13,\n        17713,   428,   530,   318,  1016,   736,    13,   198,   198,    67,\n          451, 21538,   532,  3387,   787,  1654,   326,   345,   466,   407,\n         3758,   662,    12,   322,   437,  9528,  6685,   284, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbalanced_sampler = BalancedClassRandomSampler(ds.df[ds.output_column])\n\ndl = DataLoader(ds, batch_size=64, sampler=balanced_sampler)#, shuffle=True)\nfor (x, m), y in dl:\n    print(m)\n    print(m.shape)\n    print(y)\n    print(y.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:57:39.296686Z","iopub.execute_input":"2024-04-21T21:57:39.297077Z","iopub.status.idle":"2024-04-21T21:57:39.370793Z","shell.execute_reply.started":"2024-04-21T21:57:39.297032Z","shell.execute_reply":"2024-04-21T21:57:39.369940Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])\ntorch.Size([64, 160])\ntensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0])\ntorch.Size([64])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nfrom models import EncoderOnlyClassifier\n\nnum_heads = 8\nd_model = 256\nseq_len = 160\n\nmodel = EncoderOnlyClassifier(\n    num_classes=2,\n    num_encoders = 6,\n    num_heads = 8,\n    d_model = d_model,\n    seq_len = seq_len\n)\n\nloss_fn = nn.CrossEntropyLoss()\nlr = 0.00025  # a pretty low learning rate seems to work best\nopt = torch.optim.Adam(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:57:39.371879Z","iopub.execute_input":"2024-04-21T21:57:39.372158Z","iopub.status.idle":"2024-04-21T21:57:39.623252Z","shell.execute_reply.started":"2024-04-21T21:57:39.372133Z","shell.execute_reply":"2024-04-21T21:57:39.622477Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('cpu')\nprint(f\"{device=}\")\nmodel.to(device=device)\n\nlog_loss_every = 100\n\nepochs = 5\ni = 0\nfor epoch in range(epochs):\n    for (x, m), y in dl:\n        \n        x = x.to(device=device)\n        m = m.to(device=device)\n        m = m.unsqueeze(-1) * m.unsqueeze(-2)\n        y = y.to(device=device)\n        \n        opt.zero_grad()\n\n        out = model(x, m)\n                \n        loss = loss_fn(out, y)\n        \n        loss.backward()\n                \n        opt.step()\n\n        if i % log_loss_every == 0:\n            y_pred = out.argmax(dim=-1)\n            \n            pos_ids = torch.nonzero(y)\n            neg_ids = torch.nonzero(1-y)\n\n            true_pos = (y_pred[pos_ids] == 1).float().sum()\n            false_pos = (y_pred[neg_ids] == 1).float().sum()\n            true_neg = (y_pred[neg_ids] == 0).float().sum()\n            false_neg = (y_pred[pos_ids] == 0).float().sum()\n\n            precision = true_pos / (true_pos + false_pos)\n            recall = true_pos / (true_pos + false_neg)\n            accuracy = (true_pos + true_neg) / len(y)\n\n            print(\n                f\"epoch={epoch}, iter={i}, loss={loss.detach().cpu():.4f}, \"\n                f\"accuracy={accuracy.detach().cpu():.4f}, precision={precision.detach().cpu():.4f}, \"\n                f\"recall={recall.detach().cpu():.4f}\"\n            )\n        i += 1","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:57:39.624450Z","iopub.execute_input":"2024-04-21T21:57:39.624716Z","iopub.status.idle":"2024-04-21T22:01:48.966623Z","shell.execute_reply.started":"2024-04-21T21:57:39.624692Z","shell.execute_reply":"2024-04-21T22:01:48.965781Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"device=device(type='cuda')\nepoch=0, iter=0, loss=0.7688, accuracy=0.4688, precision=0.5333, recall=0.2286\nepoch=0, iter=100, loss=0.6041, accuracy=0.6562, precision=0.6500, recall=0.7647\nepoch=0, iter=200, loss=0.4570, accuracy=0.8125, precision=1.0000, recall=0.6129\nepoch=0, iter=300, loss=0.2784, accuracy=0.8906, precision=0.8605, recall=0.9737\nepoch=1, iter=400, loss=0.2314, accuracy=0.9375, precision=0.9688, recall=0.9118\nepoch=1, iter=500, loss=0.2900, accuracy=0.8906, precision=0.9286, recall=0.8387\nepoch=1, iter=600, loss=0.4496, accuracy=0.8281, precision=0.8649, recall=0.8421\nepoch=1, iter=700, loss=0.1431, accuracy=0.9219, precision=0.9643, recall=0.8710\nepoch=2, iter=800, loss=0.3522, accuracy=0.8594, precision=0.7812, recall=0.9259\nepoch=2, iter=900, loss=0.3064, accuracy=0.8438, precision=1.0000, recall=0.6667\nepoch=2, iter=1000, loss=0.0802, accuracy=0.9844, precision=1.0000, recall=0.9677\nepoch=3, iter=1100, loss=0.0670, accuracy=0.9688, precision=1.0000, recall=0.9394\nepoch=3, iter=1200, loss=0.0569, accuracy=0.9844, precision=1.0000, recall=0.9730\nepoch=3, iter=1300, loss=0.0979, accuracy=0.9531, precision=0.9737, recall=0.9487\nepoch=3, iter=1400, loss=0.0404, accuracy=0.9844, precision=1.0000, recall=0.9655\nepoch=4, iter=1500, loss=0.0373, accuracy=1.0000, precision=1.0000, recall=1.0000\nepoch=4, iter=1600, loss=0.0186, accuracy=1.0000, precision=1.0000, recall=1.0000\nepoch=4, iter=1700, loss=0.0560, accuracy=0.9688, precision=1.0000, recall=0.9259\n","output_type":"stream"}]},{"cell_type":"code","source":"from ipywidgets import interact\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Moving stuff to CPU so we dont have to do it each time we adjust the visualization sliders\nx_ = x.cpu()\nm_ = m.cpu()\nattns = []\nfor enc_layer in model.encoder.encoder_layers:\n    layer_attns = []\n    for head in enc_layer.multihead_self_attention.attentions:\n        layer_attns.append(head.detach().cpu())\n    attns.append(layer_attns)\n\ndef visualize_attention(batch_element, attention_head, encoder_layer):\n    inp = t.convert_ids_to_tokens(x_[batch_element])\n    lim = (x_[batch_element] == 50256).to(int).argmax().item()\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.matshow(\n        attns[encoder_layer][attention_head][batch_element].detach()[:lim,:lim]\n    )\n    ax.set_yticks(np.arange(0, lim));#seq_len));\n    ax.set_yticklabels(inp[:lim]);\n    ax.set_xticks(np.arange(0, lim));#seq_len));\n    ax.set_xticklabels(inp[:lim], rotation=\"vertical\");\n\ninteract(visualize_attention, attention_head=(0,7,1), batch_element=(0,x_.shape[0]-1,1), encoder_layer=(0,5,1))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:01:48.967961Z","iopub.execute_input":"2024-04-21T22:01:48.968614Z","iopub.status.idle":"2024-04-21T22:01:51.499801Z","shell.execute_reply.started":"2024-04-21T22:01:48.968577Z","shell.execute_reply":"2024-04-21T22:01:51.498922Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=24, description='batch_element', max=49), IntSlider(value=3, description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74bc265e0adc4e05a0ea8d82e39226cf"}},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<function __main__.visualize_attention(batch_element, attention_head, encoder_layer)>"},"metadata":{}}]},{"cell_type":"code","source":"def visualize_mask(batch_element):\n    m_cpu = m.cpu().detach()\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.matshow(m_cpu[batch_element])\n\ninteract(visualize_mask, batch_element=(0,m.shape[0]-1,1))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:01:51.500906Z","iopub.execute_input":"2024-04-21T22:01:51.501181Z","iopub.status.idle":"2024-04-21T22:01:51.818240Z","shell.execute_reply.started":"2024-04-21T22:01:51.501157Z","shell.execute_reply":"2024-04-21T22:01:51.817311Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=24, description='batch_element', max=49), Output()), _dom_classes=('widg…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e296c282d14844be5ee25b9f91b577"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<function __main__.visualize_mask(batch_element)>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}